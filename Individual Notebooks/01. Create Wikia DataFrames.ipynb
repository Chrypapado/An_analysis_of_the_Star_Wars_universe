{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"Data\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"Data/Characters\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"Data/Episodes\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract every planet that characters reside\n",
    "\n",
    "Due to the fact that, in the Star Wars wikia page there isn't any available source that could classify the characters in a few important categories, it was decided that it would be most interesting to be sorted by the planet on which they reside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "planets = []\n",
    "\n",
    "page_title = \"https://starwars.fandom.com/wiki/Category:Individuals_by_planet\"\n",
    "title = f\"titles={page_title}\"\n",
    "page = requests.get(page_title)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "planet_pattern = re.compile(r'/wiki/(.*)')\n",
    "for link in soup.find_all(\"a\", {\"class\": \"category-page__member-link\"}):\n",
    "    href = link.get(\"href\")\n",
    "    name =  planet_pattern.match(href).group(1)\n",
    "    planets.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract these characters along with the information on where they reside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char = pd.DataFrame(columns = ['Name', 'Residence'])\n",
    "\n",
    "for planet in planets:\n",
    "    characters = []\n",
    "\n",
    "    page_title = \"https://starwars.fandom.com/wiki/\" + planet   \n",
    "    title = f\"titles={page_title}\"\n",
    "    page = requests.get(page_title)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    character_pattern = re.compile(r'/wiki/(.*)')\n",
    "    category_pattern = re.compile(r'Category:')\n",
    "    legend_pattern = re.compile(r'(.*)/Legends')\n",
    "    for link in soup.find_all(\"a\", {\"class\": \"category-page__member-link\"}):\n",
    "        href = link.get(\"href\")\n",
    "        name =  character_pattern.match(href).group(1)\n",
    "        if not category_pattern.match(name) and not legend_pattern.match(name):\n",
    "            characters.append(name)\n",
    "    temp_char = pd.DataFrame(characters, columns=['Name'])\n",
    "    temp_char['Residence'] = planet[9:]\n",
    "    char = char.append(temp_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add characters from planets with more than one page\n",
    "\n",
    "There are planets, where their pages are more than one and with the above method the characters on the next pages are not added. So the below code will be implemented in order to add them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_pages = {'Alderaanians' : ['https://starwars.fandom.com/wiki/Category:Alderaanians?from=Othona%2C+Dalus%0ADalus+Othona'],\n",
    "              'Corellians' : ['https://starwars.fandom.com/wiki/Category:Corellians?from=Forte%2C+Crale%0ACrale+Forte', 'https://starwars.fandom.com/wiki/Category:Corellians?from=Mullawny%2C+Stax%0AStax+Mullawny', 'https://starwars.fandom.com/wiki/Category:Corellians?from=Vergesso%2C+Ecile%0AEcile+Vergesso'],\n",
    "              'Coruscanti' : ['https://starwars.fandom.com/wiki/Category:Coruscanti?from=Jool', 'https://starwars.fandom.com/wiki/Category:Coruscanti?from=Sel%27Sabagno%2C+Elan%0AElan+Sel%27Sabagno%2FLegends'],\n",
    "              'Naboo' : ['https://starwars.fandom.com/wiki/Category:Naboo?from=Unidentified+Naboo+girl'],\n",
    "              'Tatooinians' : ['https://starwars.fandom.com/wiki/Category:Tatooinians?from=Drow%2C+Vek%0AVek+Drow', 'https://starwars.fandom.com/wiki/Category:Tatooinians?from=Kithaba%2FLegends', 'https://starwars.fandom.com/wiki/Category:Tatooinians?from=Rile%2C+Chastina%0AChastina+Rile', 'https://starwars.fandom.com/wiki/Category:Tatooinians?from=Venn%2C+Tanis%0ATanis+Venn']}\n",
    "\n",
    "for k in rest_pages.keys():\n",
    "    planet = 'Category:' + k\n",
    "    for page_title in rest_pages[k]:\n",
    "        characters = []\n",
    "        title = f\"titles={page_title}\"\n",
    "        page = requests.get(page_title)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        character_pattern = re.compile(r'/wiki/(.*)')\n",
    "        category_pattern = re.compile(r'Category:')\n",
    "        legend_pattern = re.compile(r'(.*)/Legends')\n",
    "        for link in soup.find_all(\"a\", {\"class\": \"category-page__member-link\"}):\n",
    "            href = link.get(\"href\")\n",
    "            name =  character_pattern.match(href).group(1)\n",
    "            if not category_pattern.match(name) and not legend_pattern.match(name):\n",
    "                characters.append(name)\n",
    "        temp_char = pd.DataFrame(characters, columns=['Name'])\n",
    "        temp_char['Residence'] = planet[9:]\n",
    "        char = char.append(temp_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the DataFrame for the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char['Name'] = char['Name'].str.replace('%C3%A9','é')\n",
    "char['Name'] = char['Name'].str.replace('%C3%B3','ó')\n",
    "char['Name'] = char['Name'].str.replace('%C3%A7','ç')\n",
    "char['Name'] = char['Name'].str.replace('%C3%A1','á')\n",
    "char['Name'] = char['Name'].str.replace('%60','`')\n",
    "char['Name'] = char['Name'].str.replace('%22','\"')\n",
    "char['Name'] = char['Name'].str.replace('%27',\"'\")\n",
    "char['Residence'] = char['Residence'].str.replace('%27',\"'\")\n",
    "char = char[char['Name']!='User:Hk_47/WiP2']\n",
    "char = char.drop_duplicates(subset='Name')\n",
    "char = char.sort_values(by=['Name'])\n",
    "char = char[~char['Name'].str.contains('/')]\n",
    "char = char.reset_index(drop=True)\n",
    "char['Name'] = char['Name'].str.replace('_',\" \")\n",
    "char['Residence'] = char['Residence'].str.replace('_',\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the characters directory with every character's page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_name = char['Name'].str.replace(' ',\"_\")\n",
    "\n",
    "for i in range(len(char)):\n",
    "    baseurl = \"https://starwars.fandom.com/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    title = \"titles=\" + urllib.parse.quote_plus(temp_name[i])\n",
    "    content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "    dataformat = \"format=json\"\n",
    "    query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat)\n",
    "    \n",
    "    response = urlopen(query)\n",
    "    source = response.read()\n",
    "    query_json = json.loads(source)\n",
    "    text_name = temp_name[i] + '.txt'\n",
    "    temp = './Data/Characters/' + text_name\n",
    "    with open(temp, 'w') as outfile:\n",
    "        json.dump(query_json, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add attributes to every character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(os.path.join(os.getcwd(), 'Data/Characters')):\n",
    "    with open(os.path.join(os.path.join(os.getcwd(), 'Data/Characters'), filename), 'r') as f:\n",
    "        contents = f.read()\n",
    "        query_json = ast.literal_eval(contents)\n",
    "        page_id = list(query_json['query']['pages'].keys())[0]\n",
    "        string = query_json['query']['pages'][page_id]['revisions'][0]['slots']['main']['*']\n",
    "        \n",
    "        name = filename[:-4].replace('_',' ')\n",
    "        \n",
    "        # Add homeworld attribute\n",
    "        if re.findall('homeworld=\\[\\[(.*?)\\/(?:.*?)?\\]\\]', string) != []:\n",
    "            homeworld = re.findall('homeworld=\\[\\[(.*?)\\/(?:.*?)?\\]\\]', string)\n",
    "        elif re.findall('homeworld=\\[\\[(.*?)\\]\\]', string) != []:\n",
    "            homeworld = re.findall('homeworld=\\[\\[(.*?)\\]\\]', string)\n",
    "        else:\n",
    "            homeworld = ['Unknown']\n",
    "        char.loc[char['Name']==name, 'Homeworld'] = homeworld\n",
    "        \n",
    "        # Add species attribute\n",
    "        if re.findall('species=\\[\\[(.*?)(?:\\/.*?)?(?:\\|.*?)?(?:\\(.*?)?\\]\\]', string) != []:\n",
    "            species = re.findall('species=\\[\\[(.*?)(?:\\/.*?)?(?:\\|.*?)?(?:\\(.*?)?\\]\\]', string)\n",
    "        else:\n",
    "            species = ['Unknown']\n",
    "        char.loc[char['Name']==name, 'Species'] = species[0]\n",
    "        \n",
    "        # Add gender\n",
    "        if re.findall('gender=\\[\\[Gender(?:\\/.*?)?\\|(.*?)\\]\\]', string) != []:\n",
    "            gender = re.findall('gender=\\[\\[Gender(?:\\/.*?)?\\|(.*?)\\]\\]', string)[0] \n",
    "            gender = gender.replace('Masculine Programming', 'Masculine programming').replace('Males', 'Male').replace('male', 'Male').replace('feMale', 'Female').replace('FeMale', 'Female').replace('Females', 'Female')\n",
    "        else:\n",
    "            gender = 'Unknown'\n",
    "        char.loc[char['Name']==name, 'Gender'] = gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the episodes pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = ['Episode I', 'Episode II', 'Episode III', 'Episode IV', 'Episode V', 'Episode VI']\n",
    "episodes_pages = ['Star_Wars:_Episode_I_The_Phantom_Menace',\n",
    "                  'Star_Wars:_Episode_II_Attack_of_the_Clones',\n",
    "                  'Star_Wars:_Episode_III_Revenge_of_the_Sith',\n",
    "                  'Star_Wars:_Episode_IV_A_New_Hope',\n",
    "                  'Star_Wars:_Episode_V_The_Empire_Strikes_Back',\n",
    "                  'Star_Wars:_Episode_VI_Return_of_the_Jedi']\n",
    "\n",
    "for i in range(len(episodes)):\n",
    "    baseurl = \"https://starwars.fandom.com/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    title = \"titles=\" + urllib.parse.quote_plus(episodes_pages[i])\n",
    "    content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "    dataformat = \"format=json\"\n",
    "    query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat)\n",
    "    \n",
    "    response = urlopen(query)\n",
    "    source = response.read()\n",
    "    query_json = json.loads(source)\n",
    "    text_name = episodes[i] + '.txt'\n",
    "    temp = './Data/Episodes/' + text_name\n",
    "    with open(temp, 'w') as outfile:\n",
    "        json.dump(query_json, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the characters from the six episodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(os.path.join(os.getcwd(), 'Data/Episodes')):\n",
    "    with open(os.path.join(os.path.join(os.getcwd(), 'Data/Episodes'), filename), 'r') as f:\n",
    "        contents = f.read()\n",
    "        query_json = ast.literal_eval(contents)\n",
    "        page_id = list(query_json['query']['pages'].keys())[0]\n",
    "        string = query_json['query']['pages'][page_id]['revisions'][0]['slots']['main']['*']\n",
    "        pattern = '\\[\\[(.*?)(?:\\|.*?)?\\]\\]'\n",
    "        temp = re.findall(pattern, string)\n",
    "        temp = list(char[char['Name'].isin(temp)]['Name'].drop_duplicates())\n",
    "        char[filename[:-4]] = 0\n",
    "        char.loc[char['Name'].isin(temp), filename[:-4]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rearrange columns and create trilogy columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Name', 'Homeworld', 'Residence', 'Species', 'Gender', \n",
    "           'Episode I', 'Episode II', 'Episode III', 'Episode IV', 'Episode V', 'Episode VI']\n",
    "\n",
    "char = char[columns]\n",
    "\n",
    "char.loc[:,'First Trilogy'] = 0\n",
    "char.loc[:,'Second Trilogy'] = 0\n",
    "idx = (char['Episode I']==1)|(char['Episode II']==1)|(char['Episode III']==1)\n",
    "char.loc[idx, 'First Trilogy'] = 1\n",
    "idx = (char['Episode IV']==1)|(char['Episode V']==1)|(char['Episode VI']==1)\n",
    "char.loc[idx, 'Second Trilogy'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char.to_csv('./Data/Characters.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
